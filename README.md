# Atividade-Modular -- Apache-spark

 <p>
    Este repositÃ³rio contÃ©m a soluÃ§Ã£o desenvolvida para a <strong>Atividade Modular do MÃ³dulo 3 da PÃ³s-graduaÃ§Ã£o em Engenharia de Dados</strong>
  </p>

  <p>
    O projeto utiliza <strong>Apache Spark</strong> para realizar uma anÃ¡lise exploratÃ³ria dos dados de estabelecimentos brasileiros, abordando temas como inferÃªncia de schema, formato Parquet, limpeza de dados e extraÃ§Ã£o de insights analÃ­ticos.
  </p>
<h2>ğŸ¯ Objetivos</h2>
<p>Este projeto tem como propÃ³sito exercitar, na prÃ¡tica, conceitos fundamentais do ecossistema de Big Data com o Apache Spark, incluindo:</p>
<ul>
  <li>ğŸ“„ <strong>Spark SQL</strong> para consultas, filtragens e manipulaÃ§Ã£o de grandes volumes de dados.</li>
  <li>ğŸ§© <strong>UDFs (User Defined Functions)</strong> para tratamento e enriquecimento de dados de forma personalizada.</li>
  <li>ğŸ§± Uso do <strong>formato Parquet</strong>, amplamente adotado para armazenamento eficiente em Data Lakes.</li>
  <li>âš™ï¸ AplicaÃ§Ã£o de <strong>conhecimentos gerais sobre o funcionamento do Apache Spark</strong>, incluindo schema inference, transformaÃ§Ã£o e anÃ¡lise distribuÃ­da.</li>
</ul>

<h2>ğŸ“‚ Conjunto de Dados</h2>
<p>
  Os dados utilizados foram obtidos a partir do seguinte repositÃ³rio pÃºblico:
</p>
  <p>
    ğŸ”— <a href="https://homepages.dcc.ufmg.br/~pcalais/XPE/engenharia-dados/big-data-spark/desafio/" target="_blank" rel="noopener noreferrer">
      https://homepages.dcc.ufmg.br/~pcalais/XPE/engenharia-dados/big-data-spark/desafio/
    </a>
<ul>
  <li><code>estabelecimentos.csv</code> â€“ ContÃ©m informaÃ§Ãµes cadastrais dos estabelecimentos.</li>
  <li><code>cnaes.csv</code> â€“ Lista dos CNAEs com suas descriÃ§Ãµes e cÃ³digos.</li>
</ul>
<p>
  Esses arquivos foram utilizados diretamente na anÃ¡lise com o Apache Spark, usando <code>inferSchema=true</code> para identificar automaticamente os tipos de dados.
</p>

 <h2>â“ Perguntas a serem respondidas</h2>
<ol>
  <li>Quantos estabelecimentos existem?</li>
  <li>Na tabela de estabelecimentos, quantas colunas existem e quantas sÃ£o identificadas pelo Spark como nÃºmeros? Use o <code>inferSchema</code> ao ler os arquivos.</li>
  <li>Comparando o tamanho do(s) arquivo(s) CSV original(is) com o arquivo Parquet gerado pelo Spark (<code>estabelecimentos.parquet</code>), qual Ã© a economia de espaÃ§o obtida?</li>
  <li>Quantos estabelecimentos nÃ£o tÃªm logradouro cadastrado?</li>
  <li>Quantos estabelecimentos tÃªm um logradouro cujo endereÃ§o estÃ¡ incorretamente preenchido com "AVENIDA" na coluna <code>LOGRADOURO</code>?</li>
  <li>Quantos CEPs distintos existem entre os estabelecimentos?</li>
  <li>Quantos CNAEs existem na tabela de CNAEs?</li>
  <li>Quantos estabelecimentos possuem um CNAE relacionado a cultivo?</li>
  <li>Quantos estabelecimentos sÃ£o filiais?</li>
</ol>

<h2>ğŸš€ Como Executar</h2>
<p>Para executar a anÃ¡lise, siga os passos abaixo:</p>
<ol>
  <li>Acesse o notebook no <strong>Google Colab</strong> (se disponÃ­vel) ou abra o arquivo <code>analise_estabelecimentos.py</code> no seu ambiente local.</li>
  <li>Certifique-se de que o Apache Spark e Python 3 estÃ£o instalados no seu ambiente (caso local).</li>
  <li>Se estiver usando o Colab, configure o ambiente para usar PySpark (normalmente jÃ¡ disponÃ­vel ou pode ser instalado via comandos no notebook).</li>
  <li>Garanta que os arquivos CSV estejam na pasta <code>/data</code> para que o script consiga lÃª-los corretamente.</li>
  <li>Execute as cÃ©lulas do notebook ou rode o script Python para gerar os resultados.</li>
</ol>
<p>Se desejar, vocÃª pode adaptar o script para salvar resultados ou gerar relatÃ³rios conforme sua necessidade.</p>

